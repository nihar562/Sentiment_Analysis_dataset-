{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f76c419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\NIhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NIhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\NIhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\NIhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\NIhar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "792466e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('input.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b305b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted = []\n",
    "for i in range(len(data['URL'])):\n",
    "    url = data['URL'][i]\n",
    "    headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.131 Safari/537.36'}\n",
    "    code = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(code.content,'html.parser')\n",
    "    title = soup.find('h1',class_='entry-title').text\n",
    "    extracted.append(title)\n",
    "    for j in soup.find_all('p'):\n",
    "        extracted.append(j.text)\n",
    "    with open(f'{str(i+1)}.txt', 'w',encoding='utf-8') as f:\n",
    "        for line in extracted:\n",
    "            f.write(line)\n",
    "            f.write('\\n')\n",
    "    extracted.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b4b89ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"1.txt\",encoding='utf-8') as file:\n",
    "    content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d609fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r'[^A-Za-z]',' ',text.lower())\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    return tokenized_words\n",
    "def remove_stopwords(words,stop_words):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return [x for x in words if x not in stop_words]\n",
    "    \n",
    "def count(sent):\n",
    "    score = 0\n",
    "    for x in sent:\n",
    "        score = score+1\n",
    "    return score\n",
    "\n",
    "def polarity(positive, negative):\n",
    "    return (positive - negative)/((positive + negative)+ 0.000001)\n",
    "     \n",
    "\n",
    "def subjectivity(positive_score, negative_score, num_words):\n",
    "    return (positive_score+negative_score)/(num_words+ 0.000001)\n",
    "\n",
    "\n",
    "def syllables(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "818de5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate():\n",
    "    posi = []\n",
    "    nega = []\n",
    "    neut = []\n",
    "    pola = []\n",
    "    sub = []\n",
    "    avg_sen_len = []\n",
    "    avg_len_word = []\n",
    "    syllable = []\n",
    "    comp_count = []\n",
    "    word_count = []\n",
    "    p_comp_word = []\n",
    "    f_index = []\n",
    "    Personal_pronouns = []\n",
    "    \n",
    "    for i in range(1,len(data['URL'])+1):\n",
    "        with open(f'{i}.txt',encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            \n",
    "        tokenize_word = tokenize(content)\n",
    "        l_words = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        for word in tokenize_word:\n",
    "            l_words.append(lemmatizer.lemmatize(word))\n",
    "        a_remove_stopword = remove_stopwords(l_words,stopwords.words('english'))\n",
    "        sentence = \" \".join(a_remove_stopword)\n",
    "        sia = SentimentIntensityAnalyzer()\n",
    "        d = sia.polarity_scores(sentence)\n",
    "        num_words = count(a_remove_stopword)\n",
    "        polari = polarity(d['pos'],d['neg'])\n",
    "        subje = subjectivity(d['pos'],d['neg'],num_words)\n",
    "        pola.append(polari)\n",
    "        posi.append(d['pos'])\n",
    "        nega.append(d['neg'])\n",
    "        neut.append(d['neu'])\n",
    "        sub.append(subje)\n",
    "        \n",
    "        total_sentences=content.count(\".\")\n",
    "        total_word = len(tokenize_word)\n",
    "        Average_sentence_length=total_word/total_sentences\n",
    "        avg_sen_len.append(round(Average_sentence_length))\n",
    "        \n",
    "        word_count.append(total_word)\n",
    "        \n",
    "        size_of_word=0\n",
    "        for x in content:\n",
    "            size_of_word=size_of_word+len(x)\n",
    "        Average_size_of_word=size_of_word/total_word\n",
    "        avg_len_word.append(round(Average_size_of_word))\n",
    "        \n",
    "        complex_count=0\n",
    "        syllable_per_word=[]\n",
    "        for x in content:\n",
    "            k=syllables(x)\n",
    "            syllable_per_word.append(k)\n",
    "            if k>2:\n",
    "                complex_count+=1\n",
    "        syllable_per_word=sum(syllable_per_word)\n",
    "        comp_count.append(complex_count)\n",
    "        syllable.append(syllable_per_word)\n",
    "        \n",
    "        p_of_comp_words=(complex_count/total_word)*100\n",
    "        p_comp_word.append(p_of_comp_words)\n",
    "        \n",
    "        fog_index = (Average_sentence_length+p_of_comp_words)*0.4\n",
    "        f_index.append(fog_index)\n",
    "        \n",
    "        pp_count=0\n",
    "        for x in content:\n",
    "            pattern=r\"(\\b(I|we|my|us|ours)\\b)\"\n",
    "            r1 = re.match(pattern, x)\n",
    "            if r1:\n",
    "                pp_count+=1\n",
    "        Personal_pronouns.append(pp_count)\n",
    "        \n",
    "    return [posi,nega,neut,pola,sub,avg_sen_len,avg_len_word,syllable,comp_count,word_count,p_comp_word,f_index,Personal_pronouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2eed2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive,negative,neutral,polarity,subjectivity,Average_sentence_len,Average_word_len,len_syllable,complex_count,word_count,complex_word_precentage,fog_index,Personal_pronouns = calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "765c63b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.read_excel('Output Data Structure.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb88bca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
       "       'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
       "       'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
       "       'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
       "       'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH',\n",
       "       'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18',\n",
       "       'Unnamed: 19', 'Unnamed: 20'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d44f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['POSITIVE SCORE'] = positive\n",
    "output['NEGATIVE SCORE'] = negative\n",
    "output['POLARITY SCORE'] = polarity\n",
    "output['SUBJECTIVITY SCORE'] = subjectivity\n",
    "output['AVG SENTENCE LENGTH'] = Average_sentence_len\n",
    "output['PERCENTAGE OF COMPLEX WORDS'] = complex_word_precentage\n",
    "output['FOG INDEX'] = fog_index\n",
    "output['AVG NUMBER OF WORDS PER SENTENCE'] = Average_word_len\n",
    "output['COMPLEX WORD COUNT'] = complex_count\n",
    "output['WORD COUNT'] = word_count\n",
    "output['SYLLABLE PER WORD'] = len_syllable\n",
    "output['PERSONAL PRONOUNS'] = Personal_pronouns\n",
    "output['AVG WORD LENGTH'] = Average_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d90c74aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.378046</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.711111</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>723</td>\n",
       "      <td>4375</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.627905</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.114286</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>638</td>\n",
       "      <td>4045</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.691664</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.557895</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1816</td>\n",
       "      <td>11415</td>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.657143</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>443</td>\n",
       "      <td>2808</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.639997</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.277419</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>719</td>\n",
       "      <td>4395</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  POSITIVE SCORE  \\\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...           0.113   \n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...           0.210   \n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...           0.203   \n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...           0.167   \n",
       "4       5  https://insights.blackcoffer.com/how-artificia...           0.205   \n",
       "\n",
       "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
       "0           0.051        0.378046            0.000391                   27   \n",
       "1           0.048        0.627905            0.000675                   23   \n",
       "2           0.037        0.691664            0.000223                   24   \n",
       "3           0.000        0.999994            0.000650                   32   \n",
       "4           0.045        0.639997            0.000617                   23   \n",
       "\n",
       "   PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0                          0.0  10.711111                                 6   \n",
       "1                          0.0   9.114286                                 6   \n",
       "2                          0.0   9.557895                                 6   \n",
       "3                          0.0  12.657143                                 6   \n",
       "4                          0.0   9.277419                                 6   \n",
       "\n",
       "   COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
       "0                   0         723               4375                 16   \n",
       "1                   0         638               4045                 12   \n",
       "2                   0        1816              11415                 53   \n",
       "3                   0         443               2808                  2   \n",
       "4                   0         719               4395                 28   \n",
       "\n",
       "   AVG WORD LENGTH  \n",
       "0                6  \n",
       "1                6  \n",
       "2                6  \n",
       "3                6  \n",
       "4                6  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output.drop(['Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18',\n",
    "       'Unnamed: 19', 'Unnamed: 20'],axis = 1)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd129752",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2aa375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
